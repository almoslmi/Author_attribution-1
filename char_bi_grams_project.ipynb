{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "random_forest [0.8428571428571429]\n",
      "svm [0.6857142857142857]\n",
      "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(3, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "random_forest [0.8142857142857143]\n",
      "svm [0.6142857142857143]\n",
      "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(4, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "random_forest [0.5857142857142857]\n",
      "svm [0.5714285714285714]\n",
      "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(2, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "random_forest [0.8571428571428571]\n",
      "svm [0.6714285714285714]\n",
      "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(2, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "random_forest [0.8428571428571429]\n",
      "svm [0.6857142857142857]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import svm, model_selection, tree, preprocessing, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import codecs\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import helper\n",
    "import scipy.special as sp\n",
    "import random\n",
    "\n",
    "\n",
    "path='Hindi_train/'\n",
    "\n",
    "\n",
    "bigram_vectorizer=CountVectorizer(analyzer='char_wb', ngram_range=(2,2), min_df= 0)\n",
    "trigram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3,3), min_df= 0)\n",
    "tetra_gram_vectorizer= CountVectorizer(analyzer='char_wb', ngram_range=(4,4), min_df= 0)\n",
    "bi_tri_gram_vectorizer=CountVectorizer(analyzer='char_wb', ngram_range=(2,3), min_df= 0)\n",
    "bi_tri_tetra_gram_vectorizer=CountVectorizer(analyzer='char_wb', ngram_range=(2,4), min_df= 0)\n",
    "\n",
    "\n",
    "\n",
    "vectorizers=[bigram_vectorizer,trigram_vectorizer,tetra_gram_vectorizer,bi_tri_gram_vectorizer,bi_tri_tetra_gram_vectorizer]\n",
    "\n",
    "\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    \n",
    "    \n",
    "    svm_scores=[]\n",
    "    random_forest_scores=[]\n",
    "    svm_times=[]\n",
    "    random_forest_times=[]\n",
    "\n",
    "    for i in range(5,6):\n",
    "        iterations=0\n",
    "        x=int(sp.comb(14,i))\n",
    "        if(x>20):\n",
    "            iterations=20\n",
    "        else:\n",
    "            iterations=x\n",
    "        iterations=1\n",
    "\n",
    "        x1=0\n",
    "        x2=0\n",
    "        y1=0\n",
    "        y2=0\n",
    "        for a in range(iterations):\n",
    "            random_list=random.sample(range(5),i)\n",
    "            train_data_X,train_data_Y,length=helper.prepare_data(authors_to_consider=random_list,vectorizer=vectorizer,add_features='no',path=path,no_of_authors=i)\n",
    "\n",
    "\n",
    "            helper.text_normalise(train_data_X,length)  #tf-idf normalisation\n",
    "            helper.feature_normalise(train_data_X)      #normalisation for machine learning algo\n",
    "\n",
    "            a,b,c,d=helper.learn(train_data_X,train_data_Y)\n",
    "            x1+=a\n",
    "            x2+=b\n",
    "            y1+=c\n",
    "            y2+=d\n",
    "\n",
    "        x1=x1/iterations\n",
    "        x2=x2/iterations\n",
    "        y1=y1/iterations\n",
    "        y2=y2/iterations\n",
    "\n",
    "        svm_scores+=[x1]\n",
    "        svm_times+=[x2]\n",
    "        random_forest_scores+=[y1]\n",
    "        random_forest_times+=[y2]\n",
    "        print(vectorizer)\n",
    "        print(\"random_forest\",random_forest_scores)\n",
    "        print(\"svm\",svm_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
