{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import svm, model_selection, tree, preprocessing, metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import codecs\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import scipy.special as sp\n",
    "import random\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "le=preprocessing.LabelEncoder() \n",
    "\n",
    "\n",
    "def prepare_data(authors_to_consider,vectorizer,path,add_features='no',no_of_authors=18):\n",
    "    \n",
    "    authors=os.listdir(path);\n",
    "    temp=[]\n",
    "    for i in range(len(authors_to_consider)):\n",
    "        temp+=[authors[authors_to_consider[i]]]\n",
    "    \n",
    " \n",
    "    authors=temp\n",
    "    files=[]\n",
    "\n",
    "    files=get_files(authors)\n",
    "    \n",
    "    \n",
    "    document=[]\n",
    "    length=[]\n",
    "    for file in files:\n",
    "        doc = codecs.open(file, \"r\", encoding='utf-16')\n",
    "        doc = doc.read()\n",
    "        document=document+[doc]\n",
    "        length=length+[len(doc)]\n",
    "\n",
    "\n",
    "    #transforming data into feature vector\n",
    "    X=vectorizer.fit_transform(document)\n",
    "    train_data_X=pd.DataFrame(data=X.toarray(),columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    \n",
    "    Y=[]\n",
    "    for author in authors:\n",
    "        newpath=path+author+'/'\n",
    "        x=os.listdir(newpath)\n",
    "        for every_file in x:\n",
    "            Y=Y+[author]\n",
    "\n",
    "    train_data_Y=le.fit_transform(Y)\n",
    "    \n",
    "    if(add_features=='no'):\n",
    "        return (train_data_X,train_data_Y,length)\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "    return (train_data_X,train_data_Y,length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_features(files,train_data_X):\n",
    "    #reading documents from file paths\n",
    "    document=[]\n",
    "    length=[]\n",
    "    hapax=[]\n",
    "    word_count=[]\n",
    "    no_of_english_words=[]                                          # number of english words that an author uses\n",
    "    avg_word_length=[]                                              # length of document divided by number of words\n",
    "    no_of_unique_words=[]                                           # vocabulary richness\n",
    "    freq_length_dist=[]\n",
    "    for file in files:\n",
    "        doc = codecs.open(file, \"r\", encoding='utf-16')\n",
    "        doc = doc.read()\n",
    "        words=doc.split(' ')\n",
    "        freq_dist=FreqDist(words)\n",
    "        no_of_hapax=len(freq_dist.hapaxes())\n",
    "        hapax=hapax+[no_of_hapax]\n",
    "        freq_of_different_words=[0 for i in range(16)]\n",
    "        unique_words=set(words)\n",
    "        no_of_unique_words=no_of_unique_words+[len(unique_words)]\n",
    "        no_of_words=len(words)\n",
    "        word_count = word_count + [no_of_words]\n",
    "        avg_word_length=avg_word_length+[len(doc)/no_of_words]\n",
    "        english_words=[]\n",
    "        for each_word in words:\n",
    "            if(len(each_word)>15):\n",
    "                words.remove(each_word)\n",
    "                continue;\n",
    "            freq_of_different_words[len(each_word)]=freq_of_different_words[len(each_word)]+1\n",
    "            if((len(each_word)>0) and (ord(each_word[0])<=126)):\n",
    "                english_words=english_words+[each_word]\n",
    "        freq_length_dist+=[freq_of_different_words]\n",
    "        no_of_english_words=no_of_english_words+[len(english_words)]\n",
    "        length=length+[len(doc)]\n",
    "       \n",
    "    \n",
    "    #adding additional features: no of english words per document\n",
    "    additional_df=pd.DataFrame(no_of_english_words,columns=['no_of_english_words'])\n",
    "    additional_df=additional_df.assign(avg_word_length=avg_word_length)\n",
    "    additional_df=additional_df.assign(no_of_unique_words=no_of_unique_words)\n",
    "    additional_df=additional_df.assign(hapax=hapax)\n",
    "    freq_length_dist=pd.DataFrame(freq_length_dist)\n",
    "    additional_df=pd.concat([additional_df,freq_length_dist],axis=1)\n",
    "    additional_df=additional_df.div(length,axis=0)\n",
    "    additional_df=additional_df.assign(l=length)\n",
    "    additional_df=additional_df.assign(word_count=word_count)\n",
    "    \n",
    "    if(train_data_X is None):\n",
    "        return additional_df,length\n",
    "    \n",
    "    train_data_X=pd.concat([train_data_X,additional_df],axis=1)\n",
    "    return train_data_X\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def text_normalise(train_data_X,length):\n",
    "    x,y=train_data_X.shape\n",
    "    #term frequency normalization\n",
    "    train_data_X=train_data_X.div(length,axis=0)\n",
    "\n",
    "    #calculating idf for each column\n",
    "    import math as math\n",
    "    l=[]\n",
    "    for each in train_data_X.columns:\n",
    "        document=0\n",
    "        for value in train_data_X.loc[:,each]:\n",
    "            if(value!=0):\n",
    "                document=document+1\n",
    "        data=math.log(x/document)\n",
    "        l=l+[data]\n",
    "\n",
    "    #tf idf\n",
    "    train_data_X=train_data_X.mul(l,axis=1)\n",
    "    \n",
    "def feature_normalise(train_data_X):\n",
    "    train_data_X=(train_data_X-train_data_X.mean())/(train_data_X.max()-train_data_X.min())    #normalisation for learning algo\n",
    "    columns=train_data_X.columns[train_data_X.isnull().any()]\n",
    "    train_data_X=train_data_X.drop(columns,axis=1)\n",
    "    return train_data_X\n",
    "    \n",
    "    \n",
    "def results(clf,train_data_X,train_data_Y,file):\n",
    "        start_time=time.time()\n",
    "        clf.fit(train_data_X,train_data_Y)\n",
    "        end_time=time.time()\n",
    "        file.write(str(clf.best_score_)+\",\")\n",
    "        file.write(str(end_time-start_time)+\"\\n\")\n",
    "        return clf\n",
    "\n",
    "\n",
    "def learn(train_data_X,train_data_Y,file,save_classifier='no',model_name=None):\n",
    "    \n",
    "   \n",
    "    normalised_data=feature_normalise(train_data_X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    svm_parameters = [{'kernel': ['rbf'],\n",
    "                   'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],\n",
    "                    'C': [1, 10, 100, 1000]}]\n",
    "    \n",
    "    \n",
    "    model=svm.SVC()\n",
    "    clf=GridSearchCV(model,param_grid=svm_parameters,cv=10)\n",
    "    file.write(\"svm,\")\n",
    "    clf=results(clf,normalised_data,train_data_Y,file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Forest_parameters=[{'n_estimators':[1000],'max_depth':[5,7,9]}]\n",
    "    model=RandomForestClassifier()\n",
    "    clf=GridSearchCV(model,param_grid=Forest_parameters,cv=10)\n",
    "    file.write(\"RandomForest,\")\n",
    "    clf=results(clf,train_data_X,train_data_Y,file)\n",
    "    \n",
    "    \n",
    "\n",
    "    knn_parameters=[{'n_neighbors':[3,5,7,9,11]}]\n",
    "    model=KNeighborsClassifier()\n",
    "    clf=GridSearchCV(model,param_grid=knn_parameters,cv=10)\n",
    "    file.write(\"knn,\")\n",
    "    clf=results(clf,train_data_X,train_data_Y,file)\n",
    "    \n",
    "    \n",
    "    clf=LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "    start_time=time.time()\n",
    "    scores=cross_val_score(clf,train_data_X,train_data_Y,cv=10)\n",
    "    end_time=time.time()\n",
    "    file.write(\"LogisticRegression,\")\n",
    "    file.write(str(scores.mean())+\",\")\n",
    "    file.write(str(end_time-start_time)+\"\\n\")\n",
    "    \n",
    "    \n",
    "    clf=MultinomialNB()\n",
    "    start_time=time.time()\n",
    "    scores=cross_val_score(clf,train_data_X,train_data_Y,cv=10)\n",
    "    end_time=time.time()\n",
    "    file.write(\"NaiveBayes\"+\",\")\n",
    "    file.write(str(scores.mean())+\",\")\n",
    "    file.write(str(end_time-start_time)+\"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "def get_files(authors):\n",
    "    #getting the list of files\n",
    "    path='Hindi_train/'\n",
    "    files=[]\n",
    "    for author in authors:\n",
    "        newpath=path+author+'/'\n",
    "        x=os.listdir(newpath)\n",
    "        for every_file in x:\n",
    "            full_path=newpath+every_file\n",
    "            files=files+[full_path]\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
